本研究通过分析EEG信号变化，识别和预测个体在不同状态下的神经活动规律，以脑电信号为研究对象，聚焦于如何利用深度学习模型提升脑电信号的时序预测精度，针对LSTM、ESN和Transformer三个深度学习模型进行研究对比，并搭建了一个具有交互功能的可视化预测界面。目的是通过比较不同模型对EEG信号的预测效果，再对它们优化改进，探讨更准确、更稳定的EEG信号预测办法。
研究设计包含四个关键步骤：第一，使用ThinkGear设备采集EEG原始数据，并通过串口通信实现数据的实时传输；第二，对采集到的数据进行标准化处理，并构造滑动窗口序列以满足深度模型输入格式要求；第三，集成LSTM、ESN和Transformer三类模型，分别进行结构设计和优化；第四，通过Streamlit框架搭建交互式平台，支持用户进行模型选择、结果对比与误差输出。

数据采集方面，使用 ThinkGear 脑电传感器获取额部信号，提取 delta 至 beta 共 8 个波段。通过 Python 的 serial 模块建立串口连接，调用 TG_Connect 持续接收数据，并用 TG_GetValue 提取各频段功率值。经校验的数据按时间戳实时写入 CSV，为模型训练提供输入基础。

LSTM 是为解决长序列中的梯度消失问题而设计的 RNN 变体，通过输入门、遗忘门和输出门实现长期依赖信息的有效传递。
研究对它的结构做了一些初步优化，采用双层堆叠的方式，关于激活函数本研究最初用的是ReLU激活，但效果不好。换用更平滑的GELU，虽然训练速度变慢了，但评价指标有所提高。在后续的优化中为了减少因为高维输入而出现的过拟合问题（首先想到的是PCA降维，但需要离线计算，）于是引入了自编码器，对每个时间步的特征进行压缩。经过这些调整，LSTM模型的最优MSE为0.00499，微幅优于不带编码器的原LSTM（0.00501）。

ESN 是一种基于储备池的递归神经网络，特点是仅训练输出权重，隐藏层参数固定。
一开始优化采用手动调节参数，但是优化效率低。于是采用网格搜索优化ESN超参数，重点调整节点数、谱半径与输入缩放比例等参数。最终选出的最优组合为(100, 0.9, 0.7, 1e-05)，使MSE由初始的0.00547降至0.00501。

Transformer 是一种基于自注意力机制的深度模型，其核心在于多头注意力机制，可实现序列中任意位置的信息交互。
第一次优化使用了可学习的位置编码、残差连接，实现了误差从0.01465到0.00621的飞跃，但其在进一步压缩误差方面陷入瓶颈。第二次优化采用了两个关键手段：堆叠多层Transformer Block并配合使用自适应学习作为训练过程的动态调整策略，这次优化后损失略高于前一阶段，通过加入门控注意力机制，最终 MSE 降至 0.00619。

在可视化平台部分，系统支持用户上传EEG数据并选择模型进行预测，绘制真实值与预测值的对比曲线，并自动计算MSE和MAE等误差指标，结果可一键导出为CSV。

